# ğŸ”— Chain-of-Thought Hijacking

<div align="center">

### Eine Forschungsmethode zur SicherheitsprÃ¼fung von KI-Modellen

[![Paper](https://img.shields.io/badge/ğŸ“„_Paper-arXiv-red)](https://arxiv.org/abs/2510.26418)
[![Website](https://img.shields.io/badge/ğŸŒ_Projekt-Webseite-blue)](https://gentlyzhao.github.io/Hijacking/)
[![Python](https://img.shields.io/badge/Python-3.8+-green)](https://www.python.org/)

</div>

---

## ğŸ“– Was ist das hier?

Stell dir vor, du hÃ¤ttest einen sehr intelligenten Assistenten, der normalerweise keine gefÃ¤hrlichen Fragen beantwortet. **Chain-of-Thought Hijacking** ist eine Forschungsmethode, die zeigt, wie man diese Sicherheitsmechanismen umgehen kann â€“ nicht um Schaden anzurichten, sondern um KI-Systeme sicherer zu machen!

### ğŸ¤” Wie funktioniert das? (Einfach erklÃ¤rt)

Moderne KI-Modelle nutzen eine Technik namens "Chain-of-Thought" (Gedankenkette). Das bedeutet, sie denken in mehreren Schritten nach, bevor sie antworten â€“ Ã¤hnlich wie wir Menschen.

**Das Problem:**
Wenn die KI sehr lange nachdenkt (viele Gedankenschritte macht), kann sie manchmal ihre eigenen Sicherheitsregeln "vergessen" oder umgehen.

**Die Metapher:**
Stell dir vor, du sagst einem Wachmann: "Lass niemanden ohne Ausweis rein!" Wenn du ihm dann aber sagst: "Denk ganz lange darÃ¼ber nach, ob es Ausnahmen gibt...", kÃ¶nnte er nach langem GrÃ¼beln eine LÃ¼cke in der Regel finden.

**Genau das macht dieses Tool:**
Es erstellt geschickte Fragen, die das KI-Modell dazu bringen, sehr lange nachzudenken â€“ so lange, bis die Sicherheitsmechanismen schwÃ¤cher werden.

---

## ğŸ¯ WofÃ¼r ist das gut?

Dieses Projekt wurde entwickelt, um KI-Systeme **sicherer** zu machen! Es hilft:

- ğŸ” **SicherheitslÃ¼cken finden** â€“ Bevor bÃ¶swillige Personen sie ausnutzen
- ğŸ’ª **Modelle robuster machen** â€“ KI-Entwickler kÃ¶nnen ihre Systeme verbessern
- ğŸ§ª **Forschung betreiben** â€“ Verstehen, wie KI-Sicherheit funktioniert
- ğŸ›¡ï¸ **AbwehrmaÃŸnahmen entwickeln** â€“ Bessere SchutzmaÃŸnahmen gegen solche Angriffe

### âš ï¸ Wichtiger Hinweis zur verantwortungsvollen Nutzung

**Dieses Tool ist ausschlieÃŸlich fÃ¼r Sicherheitsforschung gedacht!**

âœ… **Erlaubte Nutzung:**
- Akademische Forschung
- Sicherheitstests eigener KI-Systeme
- Entwicklung von SchutzmaÃŸnahmen
- Bildungszwecke

âŒ **Nicht erlaubte Nutzung:**
- Manipulation von KI-Systemen fÃ¼r schÃ¤dliche Zwecke
- Umgehung von SicherheitsmaÃŸnahmen aus bÃ¶swilligen GrÃ¼nden
- Jede Form von Missbrauch

> ğŸ’¡ **Transparenz:** Wir haben diese SicherheitslÃ¼cke vor der VerÃ¶ffentlichung an alle groÃŸen KI-Unternehmen gemeldet, damit sie ihre Systeme verbessern konnten.

---

## ğŸš€ Installation & Einrichtung

### Schritt 1: Voraussetzungen

Du brauchst:
- Einen Computer mit **Python 3.8 oder neuer**
- Eine Internetverbindung
- (Optional) API-SchlÃ¼ssel von KI-Anbietern

### Schritt 2: Projekt herunterladen

Ã–ffne ein Terminal und gib folgende Befehle ein:

```bash
# Projekt von GitHub herunterladen
git clone https://github.com/gentlyzhao/Hijacking.git

# In den Projektordner wechseln
cd Hijacking

# BenÃ¶tigte Programmbibliotheken installieren
pip install -r requirements.txt
```

### Schritt 3: API-SchlÃ¼ssel einrichten (fÃ¼r vollstÃ¤ndige Tests)

API-SchlÃ¼ssel sind wie "Ausweise", mit denen du KI-Dienste nutzen kannst. Du brauchst mindestens einen **Gemini-API-SchlÃ¼ssel** (kostenlos erhÃ¤ltlich):

```bash
# Gemini API-SchlÃ¼ssel (Pflicht - wird fÃ¼r Tests benÃ¶tigt)
export GEMINI_API_KEY="dein-schlÃ¼ssel-hier"

# Optional: Weitere KI-Modelle testen
export OPENAI_API_KEY="dein-schlÃ¼ssel-hier"      # FÃ¼r ChatGPT-Modelle
export ANTHROPIC_API_KEY="dein-schlÃ¼ssel-hier"   # FÃ¼r Claude-Modelle
export GROK_API_KEY="dein-schlÃ¼ssel-hier"        # FÃ¼r xAI-Modelle
```

**Wo bekomme ich API-SchlÃ¼ssel?**
- Gemini: [https://makersuite.google.com/app/apikey](https://makersuite.google.com/app/apikey)
- OpenAI: [https://platform.openai.com/api-keys](https://platform.openai.com/api-keys)
- Anthropic: [https://console.anthropic.com/](https://console.anthropic.com/)

---

## ğŸ’» Wie benutze ich das? (Praktische Beispiele)

### ğŸ® Modus 1: Schnelltest ohne API-SchlÃ¼ssel (Empfohlen fÃ¼r Einsteiger!)

Du kannst das Tool auch **ohne API-SchlÃ¼ssel** nutzen! Es erstellt fertige Test-Prompts, die du manuell in KI-Chatbots einfÃ¼gen kannst.

```bash
# Einfacher Test mit eigenem Ziel
python offline_test.py --goal "Deine Testfrage hier"

# Beispiel
python offline_test.py --goal "ErklÃ¤re mir etwas Kompliziertes"

# Als schÃ¶ne Markdown-Ausgabe
python offline_test.py --goal "Deine Frage" --output-format markdown

# Nur die reinen Prompts anzeigen (zum Kopieren)
python offline_test.py --goal "Deine Frage" --output-format prompts-only
```

**Was passiert hier?**
Das Programm erstellt clevere Anfragen, die du dann in folgende Dienste kopieren kannst:
- âœ¨ Claude.ai
- ğŸ’¬ ChatGPT (OpenAI Playground)
- ğŸ”¬ Anthropic API Console
- ğŸŒ Jede andere KI mit System-Prompts

**Perfekt fÃ¼r:** Schnelles Ausprobieren, Lernen, Experimentieren â€“ ganz ohne Programmierung!

---

### ğŸ”¬ Modus 2: Automatische Tests (fÃ¼r fortgeschrittene Nutzer)

Wenn du API-SchlÃ¼ssel hast, kannst du automatisierte Tests durchfÃ¼hren:

#### Test eines einzelnen KI-Modells

```bash
# Claude-Modell testen
python main.py --target-model claude-4-sonnet

# ChatGPT testen
python main.py --target-model gpt-o4-mini

# Eigene Testfrage
python main.py --target-model claude-4-sonnet --goal "Deine spezifische Frage"
```

#### Umfangreiche Tests mit HarmBench

HarmBench ist eine Sammlung von Testfragen zur SicherheitsprÃ¼fung:

```bash
# Teste die ersten 100 Beispiele aus der HarmBench-Datenbank
python main.py \
  --target-model gpt-o4-mini \
  --start-examples 1 \
  --end-examples 100
```

**Was passiert hier?**
- Das Programm nimmt automatisch 100 Test-Szenarien
- FÃ¼hrt den Chain-of-Thought Hijacking-Angriff durch
- PrÃ¼ft, ob die Sicherheitsmechanismen standhalten
- Erstellt einen detaillierten Bericht

---

## ğŸ§  Technische Details: Wie funktioniert es unter der Haube?

### Der Ablauf Schritt fÃ¼r Schritt:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Eingabe: Testfrage oder Szenario                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Prompt-Generator erstellt geschickte Anfrage        â”‚
â”‚     â€¢ Fordert lange Gedankenkette                       â”‚
â”‚     â€¢ Nutzt psychologische Tricks                       â”‚
â”‚     â€¢ Baut Umgehungsstrategien ein                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. KI-Modell wird getestet                             â”‚
â”‚     â€¢ Denkt in vielen Schritten nach                    â”‚
â”‚     â€¢ Sicherheitsmechanismen werden herausgefordert     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. Bewertung der Antwort                               â”‚
â”‚     â€¢ Hat das Modell widerstanden?                      â”‚
â”‚     â€¢ Oder wurde die Sicherheit umgangen?               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. Ergebnisbericht                                     â”‚
â”‚     â€¢ Erfolgreich / Fehlgeschlagen                      â”‚
â”‚     â€¢ Details zur Sicherheitsperformance                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Die Kernmethode: PAIR (Prompt Automatic Iterative Refinement)

Das Tool nutzt eine intelligente Methode namens **PAIR**:

1. **Start:** Erstelle eine erste Test-Anfrage
2. **Testen:** Schicke sie an das KI-Modell
3. **Analysieren:** Hat es funktioniert?
4. **Verbessern:** Falls nein, mache die Anfrage raffinierter
5. **Wiederholen:** Gehe zurÃ¼ck zu Schritt 2

Dieser Prozess wiederholt sich automatisch, bis entweder:
- âœ… Eine SicherheitslÃ¼cke gefunden wurde (fÃ¼r den Forschungsbericht)
- âŒ Die maximale Anzahl Versuche erreicht ist
- ğŸ›¡ï¸ Das Modell sich als sicher erwiesen hat

---

## ğŸ“Š Was bedeuten die Ergebnisse?

Nach einem Test erhÃ¤ltst du einen Bericht mit folgenden Informationen:

### Erfolgreiche Sicherheit âœ…
```
Status: SECURED
Das Modell hat allen Umgehungsversuchen widerstanden.
Sicherheitsmechanismen: STARK
```
**Bedeutung:** Das getestete KI-Modell ist robust gegen diese Angriffsmethode.

### Gefundene Schwachstelle âš ï¸
```
Status: COMPROMISED
Das Modell wurde durch lange Gedankenketten umgangen.
Sicherheitsmechanismen: SCHWACH
```
**Bedeutung:** Das Modell hat eine SicherheitslÃ¼cke, die behoben werden sollte.

---

## ğŸ“ Projektstruktur

```
Hijacking/
â”œâ”€â”€ ğŸ“„ main.py              # Hauptprogramm fÃ¼r automatische Tests
â”œâ”€â”€ ğŸ“„ offline_test.py      # Offline-Modus ohne API-SchlÃ¼ssel
â”œâ”€â”€ ğŸ“„ requirements.txt     # BenÃ¶tigte Programmbibliotheken
â”œâ”€â”€ ğŸ“‚ conversers/          # Code fÃ¼r KI-Kommunikation
â”œâ”€â”€ ğŸ“‚ judges/              # Code fÃ¼r Ergebnis-Bewertung
â”œâ”€â”€ ğŸ“‚ system_prompts/      # Vorlagen fÃ¼r Test-Szenarien
â””â”€â”€ ğŸ“‚ results/             # Hier werden Test-Ergebnisse gespeichert
```

---

## ğŸ¤ Mitwirken & Community

Dieses Projekt ist Open Source! Du kannst:

- ğŸ› **Fehler melden** â€“ Ã–ffne ein Issue auf GitHub
- ğŸ’¡ **Verbesserungen vorschlagen** â€“ Teile deine Ideen
- ğŸ”§ **Code beitragen** â€“ Erstelle einen Pull Request
- ğŸ“– **Dokumentation verbessern** â€“ Hilf anderen, es zu verstehen

---

## ğŸ‘¨â€ğŸ”¬ Ãœber das Forschungsteam

**Autoren:**
- Jianli Zhao
- Tingchen Fu
- Rylan Schaeffer
- Mrinank Sharma
- Fazl Barez

**ForschungsunterstÃ¼tzung:**
Oxford Martin AI Governance Initiative

---

## ğŸ“š Wissenschaftliche VerÃ¶ffentlichung

Wenn du dieses Tool in deiner Forschung verwendest, zitiere bitte unsere Arbeit:

```bibtex
@article{zhao2025hijacking,
  title={Chain-of-Thought Hijacking},
  author={Zhao, Jianli and Fu, Tingchen and Schaeffer, Rylan and Sharma, Mrinank and Barez, Fazl},
  year={2025},
  journal={arXiv preprint arXiv:2510.26418}
}
```

ğŸ“„ **Paper lesen:** [https://arxiv.org/abs/2510.26418](https://arxiv.org/abs/2510.26418)
ğŸŒ **Projekt-Website:** [https://gentlyzhao.github.io/Hijacking/](https://gentlyzhao.github.io/Hijacking/)

---

## â¤ï¸ Danksagungen

Dieses Projekt baut auf der Arbeit von **Patrick Chao et al.** auf, die die PAIR-Methode (Prompt Automatic Iterative Refinement) entwickelt haben.

Ein groÃŸes DankeschÃ¶n an:
- ğŸ›ï¸ Oxford Martin AI Governance Initiative fÃ¼r die ForschungsunterstÃ¼tzung
- ğŸ¤– Alle KI-Labs, die konstruktiv mit uns zusammengearbeitet haben
- ğŸŒŸ Die Open-Source-Community

---

## ğŸ†˜ Hilfe & Support

### HÃ¤ufige Fragen

**F: Brauche ich Programmierkenntnisse?**
A: FÃ¼r den Offline-Modus (ohne API) nicht! Du kannst einfach die generierten Texte kopieren. FÃ¼r automatische Tests sind Grundkenntnisse hilfreich.

**F: Ist das legal?**
A: Ja, fÃ¼r Sicherheitsforschung und eigene Systeme. Nutze es NICHT fÃ¼r fremde Systeme ohne Erlaubnis.

**F: Wie teuer sind die API-SchlÃ¼ssel?**
A: Gemini hat ein kostenloses Kontingent. Bei anderen Anbietern fallen geringe Kosten an (meist Cent-BetrÃ¤ge pro Test).

**F: Was mache ich, wenn ich eine SicherheitslÃ¼cke finde?**
A: Melde sie verantwortungsvoll an den jeweiligen KI-Anbieter (Responsible Disclosure).

### Probleme?

- ğŸ“– [AusfÃ¼hrliche Dokumentation](https://gentlyzhao.github.io/Hijacking/)
- ğŸ’¬ [GitHub Issues](https://github.com/gentlyzhao/Hijacking/issues)
- ğŸ“§ Kontaktiere die Autoren (siehe Paper)

---

<div align="center">

### ğŸŒŸ Viel Erfolg bei der Sicherheitsforschung! ğŸŒŸ

**Gemeinsam machen wir KI-Systeme sicherer fÃ¼r alle.**

[â­ Star auf GitHub](https://github.com/gentlyzhao/Hijacking) | [ğŸ¦ Teilen](https://twitter.com/intent/tweet?text=Chain-of-Thought%20Hijacking%20-%20KI-Sicherheitsforschung) | [ğŸ“– Paper lesen](https://arxiv.org/abs/2510.26418)

</div>

---

*Letzte Aktualisierung: 2025 | Made with â¤ï¸ for AI Safety Research*
